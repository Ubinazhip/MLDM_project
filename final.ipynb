{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYUepFbgjtEo",
    "outputId": "b9e066a9-c022-4332-a57a-3e0fa1de048c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category_encoders\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/57/fcef41c248701ee62e8325026b90c432adea35555cbc870aff9cfba23727/category_encoders-2.2.2-py2.py3-none-any.whl (80kB)\n",
      "\r",
      "\u001b[K     |████                            | 10kB 19.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 20kB 25.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 30kB 15.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 40kB 11.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 51kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 61kB 8.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 71kB 8.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 81kB 5.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.22.2.post1)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.18.5)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.1.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.17.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.1)\n",
      "Installing collected packages: category-encoders\n",
      "Successfully installed category-encoders-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybT8JlX9zlBf"
   },
   "source": [
    "# Sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8x7ooF9Hq5jQ"
   },
   "outputs": [],
   "source": [
    "#sources:\n",
    "#https://www.kaggle.com/yasufuminakama/moa-pytorch-nn-starter\n",
    "#https://www.kaggle.com/namanj27/new-baseline-pytorch-moa\n",
    "#https://www.kaggle.com/headsortails/explorations-of-action-moa-eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-P0DIzAzr7Gf"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TfmIwzShvb8h"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L16Vse4lk1Ww",
    "outputId": "a6b4cb20-e1e5-4a25-be4d-d9832147fcbf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from sklearn.feature_selection import VarianceThreshold as V\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2gN6u6svlh0"
   },
   "source": [
    "# Read The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NkXEe23UlGFP"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('drive/MyDrive/lish-moa/train_features.csv')\n",
    "df_test = pd.read_csv('drive/MyDrive/lish-moa/test_features.csv')\n",
    "df_ss = pd.read_csv('drive/MyDrive/lish-moa/sample_submission.csv')\n",
    "df_ts = pd.read_csv('drive/MyDrive/lish-moa/train_targets_scored.csv')\n",
    "df_tns = pd.read_csv('drive/MyDrive/lish-moa/train_targets_nonscored.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RUx2dB0MlyXn",
    "outputId": "f94b32eb-b462-41e7-8e24-24582395ece8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes:\n",
      " train: (23814, 876) \n",
      " target:(23814, 207) \n",
      " test: (3982, 876)\n"
     ]
    }
   ],
   "source": [
    "print(f'shapes:\\n train: {df_train.shape} \\n target:{df_ts.shape} \\n test: {df_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BB6sLhtxPwm"
   },
   "source": [
    "# Task\n",
    " <span style=\"color:#000000; font-family: 'Georgia'; font-size: 1.2em;\">\n",
    "Our task is multilabel classification. We are given 876 features(including id's of patients) and target consists of 206 features(each feature takes 0 or 1). <br>\n",
    "df_train is our training set and df_ss is the target dataframe. Our task is to get target values for df_test. <br>\n",
    "we additionally given df_tns - train_targets_nonscored. It is an additional (optional) binary responses for the training data. These are not predicted nor scored. I have no idea, why they gave it, so i didn't use it at all(as majority of competitors) <br>\n",
    "link to the competition: https://www.kaggle.com/c/lish-moa/data\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "kL43d1jkxNXi",
    "outputId": "65609b55-341a-4d60-ef14-5faefe14a538"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>g-8</th>\n",
       "      <th>g-9</th>\n",
       "      <th>g-10</th>\n",
       "      <th>g-11</th>\n",
       "      <th>g-12</th>\n",
       "      <th>g-13</th>\n",
       "      <th>g-14</th>\n",
       "      <th>g-15</th>\n",
       "      <th>g-16</th>\n",
       "      <th>g-17</th>\n",
       "      <th>g-18</th>\n",
       "      <th>g-19</th>\n",
       "      <th>g-20</th>\n",
       "      <th>g-21</th>\n",
       "      <th>g-22</th>\n",
       "      <th>g-23</th>\n",
       "      <th>g-24</th>\n",
       "      <th>g-25</th>\n",
       "      <th>g-26</th>\n",
       "      <th>g-27</th>\n",
       "      <th>g-28</th>\n",
       "      <th>g-29</th>\n",
       "      <th>g-30</th>\n",
       "      <th>g-31</th>\n",
       "      <th>g-32</th>\n",
       "      <th>g-33</th>\n",
       "      <th>g-34</th>\n",
       "      <th>g-35</th>\n",
       "      <th>...</th>\n",
       "      <th>c-60</th>\n",
       "      <th>c-61</th>\n",
       "      <th>c-62</th>\n",
       "      <th>c-63</th>\n",
       "      <th>c-64</th>\n",
       "      <th>c-65</th>\n",
       "      <th>c-66</th>\n",
       "      <th>c-67</th>\n",
       "      <th>c-68</th>\n",
       "      <th>c-69</th>\n",
       "      <th>c-70</th>\n",
       "      <th>c-71</th>\n",
       "      <th>c-72</th>\n",
       "      <th>c-73</th>\n",
       "      <th>c-74</th>\n",
       "      <th>c-75</th>\n",
       "      <th>c-76</th>\n",
       "      <th>c-77</th>\n",
       "      <th>c-78</th>\n",
       "      <th>c-79</th>\n",
       "      <th>c-80</th>\n",
       "      <th>c-81</th>\n",
       "      <th>c-82</th>\n",
       "      <th>c-83</th>\n",
       "      <th>c-84</th>\n",
       "      <th>c-85</th>\n",
       "      <th>c-86</th>\n",
       "      <th>c-87</th>\n",
       "      <th>c-88</th>\n",
       "      <th>c-89</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>-1.0220</td>\n",
       "      <td>-0.0326</td>\n",
       "      <td>0.5548</td>\n",
       "      <td>-0.0921</td>\n",
       "      <td>1.183</td>\n",
       "      <td>0.1530</td>\n",
       "      <td>0.5574</td>\n",
       "      <td>-0.4015</td>\n",
       "      <td>0.1789</td>\n",
       "      <td>-0.6528</td>\n",
       "      <td>-0.7969</td>\n",
       "      <td>0.6342</td>\n",
       "      <td>0.1778</td>\n",
       "      <td>-0.3694</td>\n",
       "      <td>-0.5688</td>\n",
       "      <td>-1.1360</td>\n",
       "      <td>-1.1880</td>\n",
       "      <td>0.6940</td>\n",
       "      <td>0.4393</td>\n",
       "      <td>0.2664</td>\n",
       "      <td>0.1907</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>-0.2853</td>\n",
       "      <td>0.5819</td>\n",
       "      <td>0.2934</td>\n",
       "      <td>-0.5584</td>\n",
       "      <td>-0.0916</td>\n",
       "      <td>-0.3010</td>\n",
       "      <td>-0.1537</td>\n",
       "      <td>0.2198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4805</td>\n",
       "      <td>0.4965</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.8427</td>\n",
       "      <td>0.1042</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>0.1758</td>\n",
       "      <td>1.2570</td>\n",
       "      <td>-0.5979</td>\n",
       "      <td>1.2250</td>\n",
       "      <td>-0.0553</td>\n",
       "      <td>0.7351</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.9590</td>\n",
       "      <td>0.2427</td>\n",
       "      <td>0.0495</td>\n",
       "      <td>0.4141</td>\n",
       "      <td>0.8432</td>\n",
       "      <td>0.6162</td>\n",
       "      <td>-0.7318</td>\n",
       "      <td>1.2120</td>\n",
       "      <td>0.6362</td>\n",
       "      <td>-0.4427</td>\n",
       "      <td>0.1288</td>\n",
       "      <td>1.4840</td>\n",
       "      <td>0.1799</td>\n",
       "      <td>0.5367</td>\n",
       "      <td>-0.1111</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>0.6685</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.3372</td>\n",
       "      <td>-0.4047</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>-1.152</td>\n",
       "      <td>-0.4201</td>\n",
       "      <td>-0.0958</td>\n",
       "      <td>0.4590</td>\n",
       "      <td>0.0803</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.5293</td>\n",
       "      <td>0.2839</td>\n",
       "      <td>-0.3494</td>\n",
       "      <td>0.2883</td>\n",
       "      <td>0.9449</td>\n",
       "      <td>-0.1646</td>\n",
       "      <td>-0.2657</td>\n",
       "      <td>-0.3372</td>\n",
       "      <td>0.3135</td>\n",
       "      <td>-0.4316</td>\n",
       "      <td>0.4773</td>\n",
       "      <td>0.2075</td>\n",
       "      <td>-0.4216</td>\n",
       "      <td>-0.1161</td>\n",
       "      <td>-0.0499</td>\n",
       "      <td>-0.2627</td>\n",
       "      <td>0.9959</td>\n",
       "      <td>-0.2483</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>-0.2102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.3905</td>\n",
       "      <td>0.7099</td>\n",
       "      <td>0.2912</td>\n",
       "      <td>0.4151</td>\n",
       "      <td>-0.2840</td>\n",
       "      <td>-0.3104</td>\n",
       "      <td>-0.6373</td>\n",
       "      <td>0.2887</td>\n",
       "      <td>-0.0765</td>\n",
       "      <td>0.2539</td>\n",
       "      <td>0.4443</td>\n",
       "      <td>0.5932</td>\n",
       "      <td>0.2031</td>\n",
       "      <td>0.7639</td>\n",
       "      <td>0.5499</td>\n",
       "      <td>-0.3322</td>\n",
       "      <td>-0.0977</td>\n",
       "      <td>0.4329</td>\n",
       "      <td>-0.2782</td>\n",
       "      <td>0.7827</td>\n",
       "      <td>0.5934</td>\n",
       "      <td>0.3402</td>\n",
       "      <td>0.1499</td>\n",
       "      <td>0.4420</td>\n",
       "      <td>0.9366</td>\n",
       "      <td>0.8193</td>\n",
       "      <td>-0.4236</td>\n",
       "      <td>0.3192</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 876 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id cp_type  cp_time cp_dose  ...    c-96    c-97    c-98    c-99\n",
       "0  id_000644bb2  trt_cp       24      D1  ... -0.3981  0.2139  0.3801  0.4176\n",
       "1  id_000779bfc  trt_cp       72      D1  ...  0.1522  0.1241  0.6077  0.7371\n",
       "\n",
       "[2 rows x 876 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "id": "4Xww4vwDzUYI",
    "outputId": "1e784d3f-979d-417b-9688-6901044186fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>adrenergic_receptor_agonist</th>\n",
       "      <th>adrenergic_receptor_antagonist</th>\n",
       "      <th>akt_inhibitor</th>\n",
       "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
       "      <th>alk_inhibitor</th>\n",
       "      <th>ampk_activator</th>\n",
       "      <th>analgesic</th>\n",
       "      <th>androgen_receptor_agonist</th>\n",
       "      <th>androgen_receptor_antagonist</th>\n",
       "      <th>anesthetic_-_local</th>\n",
       "      <th>angiogenesis_inhibitor</th>\n",
       "      <th>angiotensin_receptor_antagonist</th>\n",
       "      <th>anti-inflammatory</th>\n",
       "      <th>antiarrhythmic</th>\n",
       "      <th>antibiotic</th>\n",
       "      <th>anticonvulsant</th>\n",
       "      <th>antifungal</th>\n",
       "      <th>antihistamine</th>\n",
       "      <th>antimalarial</th>\n",
       "      <th>antioxidant</th>\n",
       "      <th>antiprotozoal</th>\n",
       "      <th>antiviral</th>\n",
       "      <th>apoptosis_stimulant</th>\n",
       "      <th>aromatase_inhibitor</th>\n",
       "      <th>atm_kinase_inhibitor</th>\n",
       "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
       "      <th>atp_synthase_inhibitor</th>\n",
       "      <th>atpase_inhibitor</th>\n",
       "      <th>atr_kinase_inhibitor</th>\n",
       "      <th>aurora_kinase_inhibitor</th>\n",
       "      <th>...</th>\n",
       "      <th>protein_synthesis_inhibitor</th>\n",
       "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
       "      <th>radiopaque_medium</th>\n",
       "      <th>raf_inhibitor</th>\n",
       "      <th>ras_gtpase_inhibitor</th>\n",
       "      <th>retinoid_receptor_agonist</th>\n",
       "      <th>retinoid_receptor_antagonist</th>\n",
       "      <th>rho_associated_kinase_inhibitor</th>\n",
       "      <th>ribonucleoside_reductase_inhibitor</th>\n",
       "      <th>rna_polymerase_inhibitor</th>\n",
       "      <th>serotonin_receptor_agonist</th>\n",
       "      <th>serotonin_receptor_antagonist</th>\n",
       "      <th>serotonin_reuptake_inhibitor</th>\n",
       "      <th>sigma_receptor_agonist</th>\n",
       "      <th>sigma_receptor_antagonist</th>\n",
       "      <th>smoothened_receptor_antagonist</th>\n",
       "      <th>sodium_channel_inhibitor</th>\n",
       "      <th>sphingosine_receptor_agonist</th>\n",
       "      <th>src_inhibitor</th>\n",
       "      <th>steroid</th>\n",
       "      <th>syk_inhibitor</th>\n",
       "      <th>tachykinin_antagonist</th>\n",
       "      <th>tgf-beta_receptor_inhibitor</th>\n",
       "      <th>thrombin_inhibitor</th>\n",
       "      <th>thymidylate_synthase_inhibitor</th>\n",
       "      <th>tlr_agonist</th>\n",
       "      <th>tlr_antagonist</th>\n",
       "      <th>tnf_inhibitor</th>\n",
       "      <th>topoisomerase_inhibitor</th>\n",
       "      <th>transient_receptor_potential_channel_antagonist</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  ...  wnt_inhibitor\n",
       "0  id_000644bb2  ...              0\n",
       "1  id_000779bfc  ...              0\n",
       "\n",
       "[2 rows x 207 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ts.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "T70xidzQzfXB",
    "outputId": "cbdfd3c2-2231-48ca-b677-fa48c5443108"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>g-8</th>\n",
       "      <th>g-9</th>\n",
       "      <th>g-10</th>\n",
       "      <th>g-11</th>\n",
       "      <th>g-12</th>\n",
       "      <th>g-13</th>\n",
       "      <th>g-14</th>\n",
       "      <th>g-15</th>\n",
       "      <th>g-16</th>\n",
       "      <th>g-17</th>\n",
       "      <th>g-18</th>\n",
       "      <th>g-19</th>\n",
       "      <th>g-20</th>\n",
       "      <th>g-21</th>\n",
       "      <th>g-22</th>\n",
       "      <th>g-23</th>\n",
       "      <th>g-24</th>\n",
       "      <th>g-25</th>\n",
       "      <th>g-26</th>\n",
       "      <th>g-27</th>\n",
       "      <th>g-28</th>\n",
       "      <th>g-29</th>\n",
       "      <th>g-30</th>\n",
       "      <th>g-31</th>\n",
       "      <th>g-32</th>\n",
       "      <th>g-33</th>\n",
       "      <th>g-34</th>\n",
       "      <th>g-35</th>\n",
       "      <th>...</th>\n",
       "      <th>c-60</th>\n",
       "      <th>c-61</th>\n",
       "      <th>c-62</th>\n",
       "      <th>c-63</th>\n",
       "      <th>c-64</th>\n",
       "      <th>c-65</th>\n",
       "      <th>c-66</th>\n",
       "      <th>c-67</th>\n",
       "      <th>c-68</th>\n",
       "      <th>c-69</th>\n",
       "      <th>c-70</th>\n",
       "      <th>c-71</th>\n",
       "      <th>c-72</th>\n",
       "      <th>c-73</th>\n",
       "      <th>c-74</th>\n",
       "      <th>c-75</th>\n",
       "      <th>c-76</th>\n",
       "      <th>c-77</th>\n",
       "      <th>c-78</th>\n",
       "      <th>c-79</th>\n",
       "      <th>c-80</th>\n",
       "      <th>c-81</th>\n",
       "      <th>c-82</th>\n",
       "      <th>c-83</th>\n",
       "      <th>c-84</th>\n",
       "      <th>c-85</th>\n",
       "      <th>c-86</th>\n",
       "      <th>c-87</th>\n",
       "      <th>c-88</th>\n",
       "      <th>c-89</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.5458</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>-0.5135</td>\n",
       "      <td>0.4408</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>-0.1644</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>0.2221</td>\n",
       "      <td>-0.3260</td>\n",
       "      <td>1.9390</td>\n",
       "      <td>-0.2305</td>\n",
       "      <td>-0.3670</td>\n",
       "      <td>1.304</td>\n",
       "      <td>1.4610</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.6816</td>\n",
       "      <td>-0.2304</td>\n",
       "      <td>-0.0635</td>\n",
       "      <td>-0.2030</td>\n",
       "      <td>-0.6821</td>\n",
       "      <td>-0.6242</td>\n",
       "      <td>0.1297</td>\n",
       "      <td>-0.0338</td>\n",
       "      <td>0.3372</td>\n",
       "      <td>0.2254</td>\n",
       "      <td>0.4795</td>\n",
       "      <td>0.7642</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>-0.2480</td>\n",
       "      <td>-0.1183</td>\n",
       "      <td>-0.4847</td>\n",
       "      <td>-0.0179</td>\n",
       "      <td>-0.8204</td>\n",
       "      <td>-0.5296</td>\n",
       "      <td>-1.5070</td>\n",
       "      <td>-0.0144</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1353</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.8939</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.2876</td>\n",
       "      <td>-0.3065</td>\n",
       "      <td>0.6519</td>\n",
       "      <td>-0.8156</td>\n",
       "      <td>-1.4960</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>0.0877</td>\n",
       "      <td>-1.0230</td>\n",
       "      <td>-0.0206</td>\n",
       "      <td>-0.4149</td>\n",
       "      <td>-0.6258</td>\n",
       "      <td>-0.2688</td>\n",
       "      <td>0.4403</td>\n",
       "      <td>-0.4900</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>-0.0914</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>-0.0612</td>\n",
       "      <td>-0.9128</td>\n",
       "      <td>-0.9399</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>-0.0035</td>\n",
       "      <td>-0.5184</td>\n",
       "      <td>-0.3485</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.7978</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.2067</td>\n",
       "      <td>-0.2303</td>\n",
       "      <td>-0.1193</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>-0.0502</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.1829</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>1.2080</td>\n",
       "      <td>-0.4522</td>\n",
       "      <td>-0.3652</td>\n",
       "      <td>-0.3319</td>\n",
       "      <td>-1.882</td>\n",
       "      <td>0.4022</td>\n",
       "      <td>-0.3528</td>\n",
       "      <td>0.1271</td>\n",
       "      <td>0.9303</td>\n",
       "      <td>0.3173</td>\n",
       "      <td>-1.012</td>\n",
       "      <td>-0.3213</td>\n",
       "      <td>0.0607</td>\n",
       "      <td>-0.5389</td>\n",
       "      <td>-0.8030</td>\n",
       "      <td>-1.0600</td>\n",
       "      <td>-0.0978</td>\n",
       "      <td>-0.8156</td>\n",
       "      <td>-0.6514</td>\n",
       "      <td>0.6812</td>\n",
       "      <td>0.5246</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5030</td>\n",
       "      <td>-0.1500</td>\n",
       "      <td>-0.1433</td>\n",
       "      <td>2.0910</td>\n",
       "      <td>-0.6556</td>\n",
       "      <td>-0.6012</td>\n",
       "      <td>-0.4104</td>\n",
       "      <td>-0.0580</td>\n",
       "      <td>-0.3608</td>\n",
       "      <td>0.2197</td>\n",
       "      <td>-0.7101</td>\n",
       "      <td>1.3430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7458</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>-0.3644</td>\n",
       "      <td>-1.818</td>\n",
       "      <td>-0.0358</td>\n",
       "      <td>-0.7925</td>\n",
       "      <td>-0.2693</td>\n",
       "      <td>-0.0938</td>\n",
       "      <td>-0.1833</td>\n",
       "      <td>-0.7402</td>\n",
       "      <td>-1.4090</td>\n",
       "      <td>0.1987</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>-1.3520</td>\n",
       "      <td>-0.3445</td>\n",
       "      <td>-0.0909</td>\n",
       "      <td>-0.6337</td>\n",
       "      <td>-0.5788</td>\n",
       "      <td>-0.7885</td>\n",
       "      <td>0.0996</td>\n",
       "      <td>-1.9480</td>\n",
       "      <td>-1.2720</td>\n",
       "      <td>-0.7223</td>\n",
       "      <td>-0.5838</td>\n",
       "      <td>-1.3620</td>\n",
       "      <td>-0.7671</td>\n",
       "      <td>0.4881</td>\n",
       "      <td>0.5913</td>\n",
       "      <td>-0.4333</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>-0.1190</td>\n",
       "      <td>-0.1852</td>\n",
       "      <td>-1.031</td>\n",
       "      <td>-1.3670</td>\n",
       "      <td>-0.3690</td>\n",
       "      <td>-0.5382</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>-0.4764</td>\n",
       "      <td>-1.381</td>\n",
       "      <td>-0.730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 876 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id cp_type  cp_time cp_dose  ...    c-96    c-97   c-98   c-99\n",
       "0  id_0004d9e33  trt_cp       24      D1  ...  0.0210 -0.0502  0.151 -0.775\n",
       "1  id_001897cda  trt_cp       72      D1  ...  0.0359 -0.4764 -1.381 -0.730\n",
       "\n",
       "[2 rows x 876 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLzdqSBVl_jh"
   },
   "source": [
    "# QuanTile Transformation\n",
    " <span style=\"color:#000000; font-family: 'Georgia'; font-size: 1.2em;\"> quatile transformation is one of the preprocssing steps, which makes the features to follow normal distribution. It also helps to reduce the effect of outliers</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ij8jvz5mHzq"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "num_cols = df_train.drop(columns = ['cp_time']).select_dtypes([np.number]).columns\n",
    "qt = QuantileTransformer(n_quantiles=100, random_state=42, output_distribution = 'normal')\n",
    "\n",
    "def quantileTransf(num_cols, df):\n",
    "    for col in num_cols:\n",
    "        qt.fit(df[col][:,None])\n",
    "        df[col] = qt.transform(df[col][:,None])\n",
    "    return df \n",
    "\n",
    "df_train = quantileTransf(num_cols, df_train)\n",
    "df_test = quantileTransf(num_cols, df_test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqthT4yvmPua"
   },
   "source": [
    "# Drop ctrl_vehicle\n",
    " <span style=\"color:#000000; font-family: 'Georgia'; font-size: 1.2em;\"> We drop the rows which has cp_type of ctl_vehicle, since their target values always 0, so we don't need them in the training, just in the end fill with 0 the rows with ctl_vehicle </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4aKHMj5jmV6i",
    "outputId": "23ceb907-00a4-406a-f5ed-1532da546ac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shapes: \n",
      " data(train(no ctl_vehicle) + target): (21948, 1082) \n",
      " test(no ctl_vehicle):                 (3982, 876)\n"
     ]
    }
   ],
   "source": [
    "def drop_ctlVecile(df):\n",
    "    df = df[df['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df_train = drop_ctlVecile(df_train)    \n",
    "test = drop_ctlVecile(df_test)\n",
    "\n",
    "data_with = pd.merge(df_train, df_ts, on='sig_id')\n",
    "data = data_with[data_with['cp_type'] != 'ctl_vehicle']\n",
    "print(f'New shapes: \\n data(train(no ctl_vehicle) + target): {data.shape} \\n test(no ctl_vehicle):                 {df_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhhLuQm4mbUR"
   },
   "source": [
    "# PCA - dimensionality reduction\n",
    " <span style=\"color:#000000; font-family: 'Georgia'; font-size: 1.2em;\"> One might say that we don't need PCA, when you use Neural Network, since NN itself is good dimensionality reduction method. However, here the we concatenate values after PCA to the original values since it somehow improves the accuracy. This was noticed in one of the sources, i mentioned above </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXM1txCjmezW"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "gene = data.loc[:,'g-0': 'g-771']\n",
    "gene_test = test.loc[:,'g-0': 'g-771']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_gene = scaler.fit_transform(gene)\n",
    "scaled_test_gene = scaler.transform(gene_test)\n",
    "pca = PCA(0.95)\n",
    "pca_data = pca.fit_transform(scaled_gene)\n",
    "pca_test = pca.transform(scaled_test_gene)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_AUHxVGmfgh"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "cell = data.loc[:,'c-0': 'c-99']\n",
    "cell_test = test.loc[:,'c-0': 'c-99']\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "scaled_cell = scaler.fit_transform(cell)\n",
    "scaled_test_cell = scaler.transform(cell_test)\n",
    "\n",
    "pca2 = PCA(0.95)\n",
    "pca_data2 = pca2.fit_transform(scaled_cell)\n",
    "pca_test2 = pca2.transform(scaled_test_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8HxEC13mhnI",
    "outputId": "616e3e9d-3232-470c-e121-8d2084798862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, the size of the new dataframe, which is concatenation of gene and cell features after PCA(0.95) is (21948, 685)\n"
     ]
    }
   ],
   "source": [
    "loadings = pca_data\n",
    "labels = ['PC' + str(x) + '_g' for x in range(1, pca_data.shape[1] + 1)]\n",
    "pca_gene = pd.DataFrame(loadings, columns = labels)\n",
    "loadings2 = pca_data2\n",
    "labels2 = ['PC' + str(x) + '_c' for x in range(1, pca_data2.shape[1] + 1)]\n",
    "pca_cell = pd.DataFrame(loadings2, columns = labels2)\n",
    "frames = [pca_gene, pca_cell]\n",
    "df_gene_cell = pd.concat(frames, axis = 1)\n",
    "print(f'So, the size of the new dataframe, which is concatenation of gene and cell features after PCA(0.95) is {df_gene_cell.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3UpPMMSUmjLh",
    "outputId": "550c10dc-f449-4592-a4e7-6fb20807aec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, the size of the new dataframe, which is concatenation of gene and cell features after PCA(0.95) is (3624, 685)\n"
     ]
    }
   ],
   "source": [
    "loadings = pca_test\n",
    "labels = ['PC' + str(x) + '_g' for x in range(1, pca_test.shape[1] + 1)]\n",
    "pca_test_gene = pd.DataFrame(loadings, columns = labels)\n",
    "loadings2 = pca_test2\n",
    "labels2 = ['PC' + str(x) + '_c' for x in range(1, pca_test2.shape[1] + 1)]\n",
    "pca_test_cell = pd.DataFrame(loadings2, columns = labels2)\n",
    "frames = [pca_test_gene, pca_test_cell]\n",
    "df_pca_test = pd.concat(frames, axis = 1)\n",
    "print(f'So, the size of the new dataframe, which is concatenation of gene and cell features after PCA(0.95) is {df_pca_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CB8e-wLiYf54"
   },
   "outputs": [],
   "source": [
    "data = data.merge(df_gene_cell, left_index=True,right_index=True, how='left')\n",
    "test = test.merge(df_pca_test, left_index=True,right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yP7TO9W4msGz"
   },
   "source": [
    "\n",
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbWRNYzqmtlQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NK4L9wrmu0V"
   },
   "outputs": [],
   "source": [
    "class base(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(base, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.BatchNorm1d(1560),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.utils.weight_norm(nn.Linear(1560, 1024)),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.utils.weight_norm(nn.Linear(1024, 1024)),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.utils.weight_norm(nn.Linear(1024, 206)),\n",
    "            \n",
    "            \n",
    "        \n",
    "        )\n",
    "       \n",
    "    def forward(self, cont_x):\n",
    "        x = self.model(cont_x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xivh5fyuDbF"
   },
   "source": [
    "## Train and test wrappers\n",
    " <span style=\"color:#000000; font-family: 'Georgia'; font-size: 1.2em;\"> We have to wrap our data in order to be able to use them in DataLoader</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1gjd6gkmv3z"
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,df, numeric,  labels):\n",
    "        self.numeric = df[numeric].values\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.numeric)\n",
    "    def __getitem__(self, idx):\n",
    "        numerical_tensor = torch.FloatTensor(self.numeric[idx])\n",
    "        label = torch.tensor(self.labels[idx]).float()\n",
    "        \n",
    "        return numerical_tensor, label\n",
    "\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,df, numeric):\n",
    "        self.numeric = df[numeric].values\n",
    "    def __len__(self):\n",
    "        return len(self.numeric)\n",
    "    def __getitem__(self, idx):\n",
    "        numerical_tensor = torch.FloatTensor(self.numeric[idx])\n",
    "        \n",
    "        return numerical_tensor\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-kCc_T9Lf73"
   },
   "source": [
    "# Variance Threshold\n",
    " <span style=\"color:#000000; font-family: 'Georgia'; font-size: 1.2em;\"> I thought of this as another preprocessing method, but it greatly reduced the accuracy</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YflGEZXirjRb"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.feature_selection import VarianceThreshold as V\n",
    "selector = V(0.8)\n",
    "my_cols = [i for i in test.columns if i not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]\n",
    "selected_data = selector.fit_transform(data.loc[:,my_cols])\n",
    "selected_test = selector.transform(test.loc[:,my_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZE4m-n8Xrpxw"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "labels = ['s' + str(x)  for x in range(1, selected_data.shape[1] + 1)]\n",
    "selected_data = pd.DataFrame(selected_data, columns = labels)\n",
    "selected_test = pd.DataFrame(selected_test, columns = labels)\n",
    "\n",
    "target_cols = [i for i in df_ts.columns if i != 'sig_id']\n",
    "categorical = ['cp_time', 'cp_dose']\n",
    "target_cols += categorical\n",
    "data = selected_data.merge(data[target_cols], left_index=True, right_index=True, how='left')\n",
    "test = selected_test.merge(data[categorical], left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwX78y8POmKK"
   },
   "source": [
    "# Categorical Variables\n",
    " <span style=\"color:#000000; font-family: 'Georgia'; font-size: 1.2em;\">\n",
    "we have some categorical values like cp_time, cp_dose, so i get their dummy values. I actually tried training without these categorical variables, but using their dummy variables improved the result </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqjCp7l3mxR4"
   },
   "outputs": [],
   "source": [
    "cp_time = pd.get_dummies(data['cp_time'], drop_first=True)\n",
    "cp_dose = pd.get_dummies(data['cp_dose'], drop_first=True)\n",
    "cp_time2 = pd.get_dummies(test['cp_time'], drop_first=True)\n",
    "cp_dose2 = pd.get_dummies(test['cp_dose'], drop_first=True) \n",
    "test = test.merge(cp_time2, left_index=True,right_index=True, how='left')\n",
    "test = test.merge(cp_dose2, left_index=True,right_index=True, how='left')\n",
    "data = data.merge(cp_time, left_index=True,right_index=True, how='left')\n",
    "data = data.merge(cp_dose, left_index=True,right_index=True, how='left')\n",
    "\n",
    "\n",
    "numerical =  data.select_dtypes([np.number])\n",
    "categorical = ['cp_time', 'cp_dose']\n",
    "target_cols = df_ts.drop(columns = ['sig_id']).columns \n",
    "numerical = [i for i in numerical.columns if i not in categorical]\n",
    "numerical = [i for i in numerical if i not in target_cols]\n",
    "data['cp_time'] = data['cp_time'].map({24: 0, 48: 1, 72: 2}) \n",
    "data['cp_dose'] = data['cp_dose'].map({'D1': 3, 'D2': 4}) \n",
    "test['cp_time'] = test['cp_time'].map({24: 0, 48: 1, 72: 2}) \n",
    "test['cp_dose'] = test['cp_dose'].map({'D1': 3, 'D2': 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8by07UdBu-rt"
   },
   "source": [
    "This ones simple training, validation and inference(for test set) functions \\\\\n",
    "source: https://www.kaggle.com/yasufuminakama/moa-pytorch-nn-starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYMDezWnmy3c"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def train(model, train_loader, scheduler, optimizer, device):\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "    for step, (numer, y) in enumerate(train_loader):\n",
    "        \n",
    "        numer, y = numer.to(device), y.to(device)\n",
    "        batch_size = numer.size(0)\n",
    "\n",
    "        pred = model(numer)\n",
    "        loss = nn.BCEWithLogitsLoss()(pred, y)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "       \n",
    "    #    val_preds.append(pred.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1000)\n",
    "\n",
    "        scheduler.step()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    return losses.avg\n",
    "        \n",
    "\n",
    "def validation(model, valid_loader, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    for step, (numer,  y) in enumerate(valid_loader):\n",
    "        numer,  y = numer.to(device),  y.to(device)\n",
    "        batch_size = numer.size(0)\n",
    "        with torch.no_grad():\n",
    "            pred = model(numer)\n",
    "        loss = nn.BCEWithLogitsLoss()(pred, y)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        val_preds.append(pred.sigmoid().detach().cpu().numpy())\n",
    "    val_preds = np.concatenate(val_preds)\n",
    "    \n",
    "    return losses.avg,val_preds        \n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model, device):\n",
    "\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    for step, (cont_x) in enumerate(test_loader):\n",
    "\n",
    "        cont_x = cont_x.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(cont_x)\n",
    "\n",
    "        preds.append(pred.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "\n",
    "    return preds\n",
    "       \n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiBR4NDhwLT8"
   },
   "source": [
    "# Loop\n",
    "here we use multilableStratifiedKfold to divide our data into training and validation. This is the same as StratifiedKfold, but since our task is multilabel classification, we use multilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWMZDn2cm0JB"
   },
   "outputs": [],
   "source": [
    "#!pip install iterative-stratification\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "kf =  MultilabelStratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "kf.get_n_splits(data) \n",
    "\n",
    "\n",
    "train_cols = data.loc[:,\"cp_time\": ].columns\n",
    "target_cols = df_ts.loc[:,\"5-alpha_reductase_inhibitor\": ].columns\n",
    "\n",
    "def run_different_seed(data, target_cols, numerical, categorical, seed = 42):\n",
    "    oof = np.zeros((len(data), len(target_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "    seed_everything(seed = seed)\n",
    "    \n",
    "    print(f'Seed: {seed}')\n",
    "    print('--------------------------------------')\n",
    "    for fold_num, (train_index, test_index) in enumerate(kf.split(data, data[target_cols])):\n",
    "        print(f'fold_num: {fold_num}')\n",
    "\n",
    "        X_train, X_test = data.loc[:, train_cols], data.loc[:, train_cols]\n",
    "        X_train, X_test = X_train.iloc[train_index], X_test.iloc[test_index]\n",
    "\n",
    "        y_train, y_test = data[target_cols], data[target_cols]\n",
    "        y_train, y_test = y_train.iloc[train_index].values, y_test.iloc[test_index].values\n",
    "\n",
    "        train_dataset = TrainDataset(X_train, numerical, y_train)\n",
    "        validation_dataset = TrainDataset(X_test, numerical,y_test)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size = 32, shuffle=True, \n",
    "                                  num_workers=4, pin_memory=True, drop_last=True)\n",
    "        valid_loader = DataLoader(validation_dataset, batch_size = 32, shuffle=False, \n",
    "                                  num_workers=4, pin_memory=True, drop_last=False)\n",
    "\n",
    "        model = base()\n",
    "        model.to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-2, weight_decay = 1e-6)\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                                  max_lr=1e-2, epochs = 10, steps_per_epoch=len(train_loader))\n",
    "        best_loss = np.inf\n",
    "        for epochs in range(5):\n",
    "\n",
    "            train_loss = train(model,train_loader, scheduler, optimizer, device)\n",
    "            print(f'epoch: {epochs}')\n",
    "            print(f'train: {train_loss}')\n",
    "            valid_loss, val_preds = validation(model,valid_loader, device)\n",
    "            print(f'validation: {valid_loss}')\n",
    "\n",
    "            if valid_loss < best_loss:\n",
    "                best_loss = valid_loss\n",
    "                _oof = np.zeros((len(data), len(target_cols)))\n",
    "                _oof[test_index] = val_preds\n",
    "                print(f'saving the best model')\n",
    "                torch.save(model.state_dict(), f\"fold{fold_num}_seed{seed}.pth\")\n",
    "\n",
    "        test_dataset = TestDataset(test, numerical)#, categorical)\n",
    "\n",
    "        test_loader = DataLoader(test_dataset, batch_size = 32, shuffle=False, \n",
    "                                  num_workers=4, pin_memory=True)\n",
    "        model.load_state_dict(torch.load(f\"fold{fold_num}_seed{seed}.pth\"))\n",
    "        model.to(device)\n",
    "        print('------------------------------------------------------------------------')\n",
    "        _predictions = inference_fn(test_loader, model, device)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        oof += _oof\n",
    "        predictions += _predictions / 5\n",
    "        \n",
    "    score = 0\n",
    "    for i in range(data[target_cols].values.shape[1]):\n",
    "        _score = log_loss(data[target_cols].values[:,i], oof[:,i])\n",
    "        score += _score / data[target_cols].values.shape[1]\n",
    "    print(f\"CV score: {score}\")\n",
    "    print('-------------------------------------------------------------') \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTSW48cbvlxJ"
   },
   "source": [
    "we use different seed for neural networks, it allows us to explore more weight spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QtAd0x1Izz77",
    "outputId": "fd6f2946-3c60-4685-b855-924114d13c69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 1\n",
      "--------------------------------------\n",
      "fold_num: 0\n",
      "epoch: 0\n",
      "train: 0.20798605027690148\n",
      "validation: 0.025065140463496668\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.020395829367798067\n",
      "validation: 0.018458024897784468\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.01885358028344973\n",
      "validation: 0.01785461714020283\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.017992535949324387\n",
      "validation: 0.017570902978267376\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.017311564254078225\n",
      "validation: 0.017312067488297514\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 1\n",
      "epoch: 0\n",
      "train: 0.20848177641929283\n",
      "validation: 0.021311235508472068\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.02078346256336646\n",
      "validation: 0.0185871114916166\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.01894108926789006\n",
      "validation: 0.01812619383792671\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.01807070003795254\n",
      "validation: 0.01780620363364866\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.017407719497942793\n",
      "validation: 0.017469349469573733\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 2\n",
      "epoch: 0\n",
      "train: 0.20907620779904842\n",
      "validation: 0.020982847114234288\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.02084769049224301\n",
      "validation: 0.018816959163809346\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.01900993682015823\n",
      "validation: 0.017588447139649993\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.018186598732374118\n",
      "validation: 0.017381341104716314\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.017605383164949553\n",
      "validation: 0.01697570708168445\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 3\n",
      "epoch: 0\n",
      "train: 0.20805258588715844\n",
      "validation: 0.020455876312691693\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.02067731244890631\n",
      "validation: 0.019005252307191797\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.018876753077862688\n",
      "validation: 0.01793945370101983\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.018054068248283907\n",
      "validation: 0.017638712945756987\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.01744447959532594\n",
      "validation: 0.017385011844353957\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 4\n",
      "epoch: 0\n",
      "train: 0.20862366762148202\n",
      "validation: 0.0212712003880215\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.020509790051320607\n",
      "validation: 0.018681770471542375\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.0188235188885354\n",
      "validation: 0.017980933098148365\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.018001112592386178\n",
      "validation: 0.017506992307106044\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.017422660321921763\n",
      "validation: 0.017505118389325128\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "CV score: 0.017329459032428267\n",
      "-------------------------------------------------------------\n",
      "Seed: 2\n",
      "--------------------------------------\n",
      "fold_num: 0\n",
      "epoch: 0\n",
      "train: 0.20752397620452256\n",
      "validation: 0.02089979503312252\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.02049525940695601\n",
      "validation: 0.018731423587472536\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.018741566074305098\n",
      "validation: 0.01797252378356484\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.017949974167765708\n",
      "validation: 0.017500607286217138\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.017338365024078067\n",
      "validation: 0.017552884780125087\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 1\n",
      "epoch: 0\n",
      "train: 0.20795054080467806\n",
      "validation: 0.02099770820171388\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.02048278871896493\n",
      "validation: 0.018669110354978022\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.018804809909626623\n",
      "validation: 0.018142488310304482\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.017978219645111447\n",
      "validation: 0.017690142311138825\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.01739902731649795\n",
      "validation: 0.017361482713108455\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 2\n",
      "epoch: 0\n",
      "train: 0.20802391102421947\n",
      "validation: 0.020889993554575756\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.020655081756956822\n",
      "validation: 0.01821669804683054\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.018942597566893066\n",
      "validation: 0.01766328115545011\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.018121765578084075\n",
      "validation: 0.01727984798849342\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.01753401681508896\n",
      "validation: 0.017061150543676896\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 3\n",
      "epoch: 0\n",
      "train: 0.2067282451996512\n",
      "validation: 0.02227203509598495\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.02055344405672411\n",
      "validation: 0.018688230690862433\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.018870349616546482\n",
      "validation: 0.017907841517621807\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.018019506296510026\n",
      "validation: 0.017553182765586907\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.017388852136413547\n",
      "validation: 0.01721084165452621\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 4\n",
      "epoch: 0\n",
      "train: 0.20944883598245845\n",
      "validation: 0.02087745818528683\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.020583087629400684\n",
      "validation: 0.01869876000934295\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.018820457420495415\n",
      "validation: 0.01818671640845584\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.01804861256064181\n",
      "validation: 0.017732567101883357\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.017442616358752886\n",
      "validation: 0.01737524669339015\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "CV score: 0.01730187366374439\n",
      "-------------------------------------------------------------\n",
      "Seed: 3\n",
      "--------------------------------------\n",
      "fold_num: 0\n",
      "epoch: 0\n",
      "train: 0.2078159554032133\n",
      "validation: 0.0215544878906127\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.020444825702911094\n",
      "validation: 0.018685343020631674\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.01876428923705579\n",
      "validation: 0.017827014338674875\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.017961389078405162\n",
      "validation: 0.017609551884997677\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.01735649192297872\n",
      "validation: 0.017295505100509027\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 1\n",
      "epoch: 0\n",
      "train: 0.20838729024823946\n",
      "validation: 0.02084798141635089\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.020796178559684297\n",
      "validation: 0.018987126224981898\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.019073256842085044\n",
      "validation: 0.018073396568859087\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.018241814211908267\n",
      "validation: 0.017779283974837332\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.01763854481035123\n",
      "validation: 0.01746260467110571\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 2\n",
      "epoch: 0\n",
      "train: 0.20987693987698397\n",
      "validation: 0.02131943120015329\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.021138186559733683\n",
      "validation: 0.019209773934559374\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.019294262025845204\n",
      "validation: 0.01786754594122285\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.018342835346005695\n",
      "validation: 0.0176500913284517\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.017812266096092055\n",
      "validation: 0.017201833408067035\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 3\n",
      "epoch: 0\n",
      "train: 0.2086456563938953\n",
      "validation: 0.021081537455490077\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.02078683621311275\n",
      "validation: 0.018846370239056866\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.018995070352429783\n",
      "validation: 0.01815330367280777\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.01819678356048454\n",
      "validation: 0.017714984576665458\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.01757267588430023\n",
      "validation: 0.017342910968225475\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 4\n",
      "epoch: 0\n",
      "train: 0.20907613196083935\n",
      "validation: 0.020677023463920488\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.020576586795590112\n",
      "validation: 0.018616128964555455\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.018815079827990085\n",
      "validation: 0.018046306375746005\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.018030291181873447\n",
      "validation: 0.01771577835247671\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.017305541166655013\n",
      "validation: 0.017436395630564674\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "CV score: 0.017347852109642493\n",
      "-------------------------------------------------------------\n",
      "Seed: 4\n",
      "--------------------------------------\n",
      "fold_num: 0\n",
      "epoch: 0\n",
      "train: 0.20931448809884107\n",
      "validation: 0.020673086775513473\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.02051018259018986\n",
      "validation: 0.018494123349004427\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.018717660682520617\n",
      "validation: 0.017977217523750248\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.017970617673611338\n",
      "validation: 0.01755557717810595\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.017394775426164813\n",
      "validation: 0.017246872453965195\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 1\n",
      "epoch: 0\n",
      "train: 0.20751091092824936\n",
      "validation: 0.020863436210841146\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.020439310585672076\n",
      "validation: 0.01893166828230193\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.018801775460138264\n",
      "validation: 0.018014065254318416\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.017881501841623963\n",
      "validation: 0.017627962139819098\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.017268871467043884\n",
      "validation: 0.017443500068844858\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 2\n",
      "epoch: 0\n",
      "train: 0.20742160322851616\n",
      "validation: 0.020656812703714375\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.020822831191611987\n",
      "validation: 0.01836895452369676\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.019007187165034405\n",
      "validation: 0.017742368734916376\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.018138611983222357\n",
      "validation: 0.017236861795624968\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.01754781001342637\n",
      "validation: 0.017078459336691658\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 3\n",
      "epoch: 0\n",
      "train: 0.20901635809917085\n",
      "validation: 0.021467924275878892\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.020801012883520258\n",
      "validation: 0.01907809563320563\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.019062776293564777\n",
      "validation: 0.018107133011645498\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.018215925215206444\n",
      "validation: 0.017978237941861153\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.017739306039170084\n",
      "validation: 0.017411887964365286\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 4\n",
      "epoch: 0\n",
      "train: 0.20876732851161084\n",
      "validation: 0.022012996133166502\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.0206519850890023\n",
      "validation: 0.018674993525963895\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.018929230834418623\n",
      "validation: 0.01801685540620505\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.0180015532626179\n",
      "validation: 0.017661433089350507\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.01746711865352997\n",
      "validation: 0.017323175815478778\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "CV score: 0.01730078850904145\n",
      "-------------------------------------------------------------\n",
      "Seed: 5\n",
      "--------------------------------------\n",
      "fold_num: 0\n",
      "epoch: 0\n",
      "train: 0.20753379144032832\n",
      "validation: 0.020851050892628408\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.0205091596801976\n",
      "validation: 0.01857388125550095\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.018790332415229538\n",
      "validation: 0.018017539539393505\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.01797773751572971\n",
      "validation: 0.017502223965892218\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.01740913953713692\n",
      "validation: 0.017287454903040903\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 1\n",
      "epoch: 0\n",
      "train: 0.20756118567184592\n",
      "validation: 0.02092744058129592\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.020705831352702892\n",
      "validation: 0.018917611292255766\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.01891657243564344\n",
      "validation: 0.01811173358563003\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.018069156802456528\n",
      "validation: 0.01776838766703709\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.017401408291945274\n",
      "validation: 0.01762591937476247\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 2\n",
      "epoch: 0\n",
      "train: 0.20753645548515404\n",
      "validation: 0.02055463058108837\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.02081983591205556\n",
      "validation: 0.01864015551482994\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.01907606367837556\n",
      "validation: 0.017656257398355236\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.01822942655268431\n",
      "validation: 0.017361367031156094\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.017697073631396477\n",
      "validation: 0.017140863328963335\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 3\n",
      "epoch: 0\n",
      "train: 0.2084251556266779\n",
      "validation: 0.021259189423402783\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.020762685484110113\n",
      "validation: 0.01897210416963201\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.018824058171891925\n",
      "validation: 0.01805271302988426\n",
      "saving the best model\n",
      "epoch: 3\n",
      "train: 0.018016110451375373\n",
      "validation: 0.017649420555299125\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.017414087718556615\n",
      "validation: 0.01729322322060891\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "fold_num: 4\n",
      "epoch: 0\n",
      "train: 0.20966924496851588\n",
      "validation: 0.021641656926581217\n",
      "saving the best model\n",
      "epoch: 1\n",
      "train: 0.02078127435809613\n",
      "validation: 0.018744727269011958\n",
      "saving the best model\n",
      "epoch: 2\n",
      "train: 0.018902280320325038\n",
      "validation: 0.01914083769646388\n",
      "epoch: 3\n",
      "train: 0.01808661481460733\n",
      "validation: 0.017691038790211763\n",
      "saving the best model\n",
      "epoch: 4\n",
      "train: 0.017521089389852255\n",
      "validation: 0.017436729738135277\n",
      "saving the best model\n",
      "------------------------------------------------------------------------\n",
      "CV score: 0.01735684447566152\n",
      "-------------------------------------------------------------\n",
      "average CV score: 0.0168916202181469\n"
     ]
    }
   ],
   "source": [
    "seeds = [1, 2, 3, 4, 5]\n",
    "oof = np.zeros((len(data), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "for s in seeds:\n",
    "    _oof, _predictions = run_different_seed(data,target_cols, numerical, categorical, seed = s)\n",
    "    oof += _oof / len(seeds)\n",
    "    predictions += _predictions / len(seeds)\n",
    "score = 0\n",
    "for i in range(data[target_cols].values.shape[1]):\n",
    "    _score = log_loss(data[target_cols].values[:,i], oof[:,i])\n",
    "    score += _score / data[target_cols].values.shape[1]\n",
    "print(f\"average CV score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "mA1RZ6ywchc9",
    "outputId": "1f0818cd-c62c-4d1d-b263-5923a7d93241"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>adrenergic_receptor_agonist</th>\n",
       "      <th>adrenergic_receptor_antagonist</th>\n",
       "      <th>akt_inhibitor</th>\n",
       "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
       "      <th>alk_inhibitor</th>\n",
       "      <th>ampk_activator</th>\n",
       "      <th>analgesic</th>\n",
       "      <th>androgen_receptor_agonist</th>\n",
       "      <th>androgen_receptor_antagonist</th>\n",
       "      <th>anesthetic_-_local</th>\n",
       "      <th>angiogenesis_inhibitor</th>\n",
       "      <th>angiotensin_receptor_antagonist</th>\n",
       "      <th>anti-inflammatory</th>\n",
       "      <th>antiarrhythmic</th>\n",
       "      <th>antibiotic</th>\n",
       "      <th>anticonvulsant</th>\n",
       "      <th>antifungal</th>\n",
       "      <th>antihistamine</th>\n",
       "      <th>antimalarial</th>\n",
       "      <th>antioxidant</th>\n",
       "      <th>antiprotozoal</th>\n",
       "      <th>antiviral</th>\n",
       "      <th>apoptosis_stimulant</th>\n",
       "      <th>aromatase_inhibitor</th>\n",
       "      <th>atm_kinase_inhibitor</th>\n",
       "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
       "      <th>atp_synthase_inhibitor</th>\n",
       "      <th>atpase_inhibitor</th>\n",
       "      <th>atr_kinase_inhibitor</th>\n",
       "      <th>aurora_kinase_inhibitor</th>\n",
       "      <th>...</th>\n",
       "      <th>protein_synthesis_inhibitor</th>\n",
       "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
       "      <th>radiopaque_medium</th>\n",
       "      <th>raf_inhibitor</th>\n",
       "      <th>ras_gtpase_inhibitor</th>\n",
       "      <th>retinoid_receptor_agonist</th>\n",
       "      <th>retinoid_receptor_antagonist</th>\n",
       "      <th>rho_associated_kinase_inhibitor</th>\n",
       "      <th>ribonucleoside_reductase_inhibitor</th>\n",
       "      <th>rna_polymerase_inhibitor</th>\n",
       "      <th>serotonin_receptor_agonist</th>\n",
       "      <th>serotonin_receptor_antagonist</th>\n",
       "      <th>serotonin_reuptake_inhibitor</th>\n",
       "      <th>sigma_receptor_agonist</th>\n",
       "      <th>sigma_receptor_antagonist</th>\n",
       "      <th>smoothened_receptor_antagonist</th>\n",
       "      <th>sodium_channel_inhibitor</th>\n",
       "      <th>sphingosine_receptor_agonist</th>\n",
       "      <th>src_inhibitor</th>\n",
       "      <th>steroid</th>\n",
       "      <th>syk_inhibitor</th>\n",
       "      <th>tachykinin_antagonist</th>\n",
       "      <th>tgf-beta_receptor_inhibitor</th>\n",
       "      <th>thrombin_inhibitor</th>\n",
       "      <th>thymidylate_synthase_inhibitor</th>\n",
       "      <th>tlr_agonist</th>\n",
       "      <th>tlr_antagonist</th>\n",
       "      <th>tnf_inhibitor</th>\n",
       "      <th>topoisomerase_inhibitor</th>\n",
       "      <th>transient_receptor_potential_channel_antagonist</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.011179</td>\n",
       "      <td>0.018964</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.006558</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.011284</td>\n",
       "      <td>0.017505</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.006480</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.004102</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>0.003091</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.002213</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003062</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.012772</td>\n",
       "      <td>0.012890</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>0.003114</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.017388</td>\n",
       "      <td>0.002069</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.003382</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.001354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.006468</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>0.009283</td>\n",
       "      <td>0.010339</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.006328</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.002566</td>\n",
       "      <td>0.002664</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.004638</td>\n",
       "      <td>0.002814</td>\n",
       "      <td>0.008467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.003014</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.005070</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>0.008173</td>\n",
       "      <td>0.006312</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.007256</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>0.002605</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.006695</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.012968</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.014801</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.001103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.008201</td>\n",
       "      <td>0.011683</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>0.001780</td>\n",
       "      <td>0.002980</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.005783</td>\n",
       "      <td>0.014229</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.002277</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.011179</td>\n",
       "      <td>0.016573</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.007527</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.002468</td>\n",
       "      <td>0.008787</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.001132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.011830</td>\n",
       "      <td>0.016090</td>\n",
       "      <td>0.002875</td>\n",
       "      <td>0.003637</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.010440</td>\n",
       "      <td>0.016321</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.012212</td>\n",
       "      <td>0.010622</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.011636</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.002066</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.012949</td>\n",
       "      <td>0.016678</td>\n",
       "      <td>0.003417</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.003474</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.013918</td>\n",
       "      <td>0.020073</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.003167</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.003640</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004610</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.002254</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.015219</td>\n",
       "      <td>0.016266</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.016363</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.002560</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.001263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  ...  wnt_inhibitor\n",
       "0  id_0004d9e33  ...       0.001354\n",
       "1  id_001897cda  ...       0.001103\n",
       "2  id_002429b5b  ...       0.001132\n",
       "3  id_00276f245  ...       0.000951\n",
       "4  id_0027f1083  ...       0.001263\n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[target_cols] = predictions\n",
    "cols = [i for i in df_ts.columns]\n",
    "cols = [i for i in df_ts.columns if i != 'sig_id']\n",
    "sub = df_ss.drop(columns=[i for i in cols]).merge(test[cols], left_index=True,right_index=True, how='left').fillna(0)\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhV_fDg4me3W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5Zuy5BGmclf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "base.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
